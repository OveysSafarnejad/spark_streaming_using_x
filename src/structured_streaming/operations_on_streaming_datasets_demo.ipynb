{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on Streaming Dataframes/Datasets Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/23 16:31:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import findspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, current_timestamp, window\n",
    "from pyspark.sql.types import StructType, TimestampType\n",
    "\n",
    "prj_dir = pathlib.Path().resolve().parent.parent\n",
    "spark_home = os.path.join(prj_dir / 'spark-3.5.0-bin-hadoop3')\n",
    "findspark.init(spark_home)\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"UserInteractionAnalyzer\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_dir = 'monitoring_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the csv files written atomically in a directory\n",
    "# The schema is as follows:\n",
    "# userA, userB, timestamp, interaction\n",
    "userSchema = StructType()\\\n",
    "    .add(\"userA\", \"integer\")\\\n",
    "    .add(\"userB\", \"integer\")\\\n",
    "    .add(\"timestamp\", TimestampType())\\\n",
    "    .add(\"interaction\", \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "activity = spark\\\n",
    "    .readStream\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .schema(userSchema)\\\n",
    "    .csv(staging_dir + \"/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_b = activity\\\n",
    "    .select(\"userB\")\\\n",
    "    .where(\"interaction = \\\"MT\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/23 16:37:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/11/23 16:37:20 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/temporary-1438e02c-021e-4952-bcc2-77311d3487d2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/11/23 16:37:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+\n",
      "| userB|\n",
      "+------+\n",
      "|   383|\n",
      "|  1454|\n",
      "| 38034|\n",
      "| 29600|\n",
      "|    88|\n",
      "|  4022|\n",
      "|261475|\n",
      "| 33476|\n",
      "|146171|\n",
      "| 59195|\n",
      "| 20802|\n",
      "| 14376|\n",
      "|    88|\n",
      "| 50901|\n",
      "| 31957|\n",
      "| 16783|\n",
      "|    88|\n",
      "|406395|\n",
      "| 68285|\n",
      "| 30978|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = user_b \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"checkpointLocation\", \"applicationHistory\") \\\n",
    "    .option(\"path\",staging_dir + \"/out\") \\\n",
    "    .start()\n",
    "\n",
    "query2 = user_b\\\n",
    "    .writeStream\\\n",
    "    .trigger(processingTime='10 seconds')\\\n",
    "    .format(\"console\")\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "# spark.sql(\"select * from aggregates\").show()   # interactively query in-memory table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
