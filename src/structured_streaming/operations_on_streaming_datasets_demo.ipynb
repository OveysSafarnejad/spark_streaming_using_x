{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on Streaming Dataframes/Datasets Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/25 20:51:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/25 20:51:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "findspark.init('/opt/spark-3.5.0/')\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"UserInteractionAnalyzer\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_dir = 'monitoring_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the csv files written atomically in a directory\n",
    "# The schema is as follows:\n",
    "# userA, userB, timestamp, interaction\n",
    "user_schema = StructType()\\\n",
    "    .add(\"userA\", \"integer\")\\\n",
    "    .add(\"userB\", \"integer\")\\\n",
    "    .add(\"timestamp\", 'timestamp')\\\n",
    "    .add(\"interaction\", \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "activity = spark\\\n",
    "    .readStream\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .schema(user_schema)\\\n",
    "    .csv(staging_dir + \"/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_ab = activity.select(['userA', 'userB']).where('interaction = \"MT\"')\n",
    "user_b = activity\\\n",
    "    .select(\"userB\")\\\n",
    "    .where(\"interaction = \\\"MT\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/25 20:51:15 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/v4/r34w0mtd5xzfzyxp6hnbp8y00000gn/T/temporary-b9c0ecdd-a682-4517-910d-79a93c3d2783. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/25 20:51:15 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+\n",
      "| userB|\n",
      "+------+\n",
      "|   383|\n",
      "|  1454|\n",
      "| 38034|\n",
      "| 29600|\n",
      "|    88|\n",
      "|  4022|\n",
      "|261475|\n",
      "| 33476|\n",
      "|146171|\n",
      "| 59195|\n",
      "| 20802|\n",
      "| 14376|\n",
      "|    88|\n",
      "| 50901|\n",
      "| 31957|\n",
      "| 16783|\n",
      "|    88|\n",
      "|406395|\n",
      "| 68285|\n",
      "| 30978|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query = user_b \\\n",
    "#     .writeStream \\\n",
    "#     .trigger(processingTime='10 seconds') \\\n",
    "#     .format(\"parquet\") \\\n",
    "#     .option(\"checkpointLocation\", \"applicationHistory\") \\\n",
    "#     .option(\"path\",staging_dir + \"/out\") \\\n",
    "#     .start()\n",
    "\n",
    "query2 = user_b\\\n",
    "    .writeStream\\\n",
    "    .trigger(processingTime='10 seconds')\\\n",
    "    .format(\"console\")\\\n",
    "    .start()\n",
    "\n",
    "query2.awaitTermination()\n",
    "# spark.sql(\"select * from aggregates\").show()   # interactively query in-memory table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
