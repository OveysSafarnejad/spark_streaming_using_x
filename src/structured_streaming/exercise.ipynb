{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on Streaming Dataframes/Datasets Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Dataset:\n",
    "\n",
    "| time              | oId   | cId   | qty | price | buy  |\n",
    "|-------------------|-------|-------|-----|-------|------|\n",
    "| 3/18/2018 2:15:18 | 34626 | 39835 |  5  | 4.72  | buy  |\n",
    "| 3/18/2018 2:24:31 | 84260 | 5443  |  9  | 15.26 | buy  |\n",
    "| 3/18/2018 2:33:44 | 56050 | 77178 |  8  | 4.07  | buy  |\n",
    "| 3/18/2018 2:42:57 | 32973 | 34441 |  5  | 15.49 | sell |\n",
    "| 3/18/2018 2:52:10 | 57264 | 98905 |  8  | 1.31  | sell |\n",
    "| 3/18/2018 3:01:23 | 21039 | 5821  |  9  | 18.85 | buy  |\n",
    "| 3/18/2018 3:10:36 | 31880 | 86234 |  6  | 19.22 | buy  |\n",
    "| 3/18/2018 3:19:49 | 82931 | 29797 |  9  | 18.10 | buy  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/25 21:55:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/25 21:55:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, current_timestamp, window\n",
    "from pyspark.sql.types import StructType, TimestampType, StructField, IntegerType, FloatType, StringType\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StreamingBuyCounter\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add in the schema for the data from buys.csv = \n",
    "data_schema = StructType() \\\n",
    "    .add(StructField('time', TimestampType())) \\\n",
    "    .add(StructField('old', IntegerType())) \\\n",
    "    .add(StructField('cld', IntegerType())) \\\n",
    "    .add(StructField('qty', IntegerType())) \\\n",
    "    .add(StructField('price', FloatType())) \\\n",
    "    .add(StructField('type', StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: read the Stream from a csv file, using the struct defined above\n",
    "filestream = spark.readStream.option('header', True).schema(data_schema).csv('data/*.csv')\n",
    "\n",
    "# TODO: use groupBy on the \"buy\" column to count the number of buys\n",
    "buySellCount = filestream.groupBy('type').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/25 21:59:25 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/v4/r34w0mtd5xzfzyxp6hnbp8y00000gn/T/temporary-91bdc76a-c131-420e-993c-dd79e6209986. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/25 21:59:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "|type|count|\n",
      "+----+-----+\n",
      "| buy|  154|\n",
      "|sell|   44|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define the query, and output the calculations to the console in \"complete\" mode\n",
    "query = buySellCount.writeStream.format(\"console\").outputMode(\"complete\").start()\n",
    "    \n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
