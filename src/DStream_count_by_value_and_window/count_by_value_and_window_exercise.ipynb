{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# countByValueAndWindow Transformation Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Streaming also provides windowed computations, which allow you to apply transformations over a sliding window of data. The following figure illustrates this sliding window.\n",
    "\n",
    "\n",
    "Some of the common window operations are as follows. All of these operations take the said two parameters - windowLength and slideInterval.\n",
    "\n",
    "| Transformation        | Meaning           |\n",
    "| -------------:|:-------------|\n",
    "| **countByValueAndWindow**(windowLength, slideInterval, [numTasks])     | When called on a DStream of (K, V) pairs, returns a new DStream of (K, Long) pairs where the value of each key is its frequency within a sliding window. Like in reduceByKeyAndWindow, the number of reduce tasks is configurable through an optional argument. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/17 10:25:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark-3.5.0-bin-hadoop3/python/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import findspark\n",
    "\n",
    "os.environ['SPARK_HOME'] = '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark-3.5.0-bin-hadoop3'\n",
    "os.environ['PYSPARK_DEIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkConf\n",
    "from apache_log_parser import ApacheAccessLog\n",
    "\n",
    "sc = SparkContext()\n",
    "ssc = StreamingContext(sparkContext=sc, batchDuration=2)\n",
    "ssc.checkpoint('checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DStream from text file\n",
    "# Note: the spark streaming checks for any updates to this directory.\n",
    "# So first, start this program, and then copy the log file logs/access_log.log to 'directory' location\n",
    "\n",
    "curr = pathlib.Path().resolve()\n",
    "logs_directory = os.path.join(curr / 'logs')\n",
    "log_data = ssc.textFileStream(logs_directory)\n",
    "access_log_dstream = log_data.map(ApacheAccessLog.parse_from_log_line).filter(lambda parsed_line: parsed_line is not None)\n",
    "ip_dstream = access_log_dstream.map(lambda parsed_line: (parsed_line.ip, 1)) \n",
    "ip_count = ip_dstream.reduceByKey(lambda x,y: x+y)\n",
    "# ip_count.pprint(num = 30)\n",
    "ip_bytes_dstream = access_log_dstream.map(lambda parsed_line: (parsed_line.ip, parsed_line.content_size))\n",
    "ip_bytes_sum_dstream = ip_bytes_dstream.reduceByKey(lambda x,y: x+y)\n",
    "ip_bytes_request_count_dstream = ip_count.join(ip_bytes_sum_dstream)\n",
    "# ip_bytes_request_count_dstream.pprint(num = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TODO: use reduceByKeyAndWindow() to get Ip counts per window ###########\n",
    "access_log = access_log_dstream.map(lambda pl: pl.ip)\n",
    "access_log.countByValueAndWindow(\n",
    "    windowDuration=20,\n",
    "    slideDuration=10\n",
    ").pprint()\n",
    "\n",
    "####### Exercise End ##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-11-17 10:26:44\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot parse logline: h194n2fls308o1033.telia.com - - [09/Mar/2004:13:49:05 -0800] \"-\" 408 -\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2023-11-17 10:26:54\n",
      "-------------------------------------------\n",
      "('80-219-148-207.dclient.hispeed.ch', 1)\n",
      "('mmscrm07-2.sac.overture.com', 3)\n",
      "('h24-70-56-49.ca.shawcable.net', 7)\n",
      "('prxint-sxb2.e-i.net', 1)\n",
      "('lj1027.inktomisearch.com', 2)\n",
      "('pool-68-160-195-60.ny325.east.verizon.net', 5)\n",
      "('lj1052.inktomisearch.com', 1)\n",
      "('fw.kcm.org', 2)\n",
      "('h24-71-236-129.ca.shawcable.net', 51)\n",
      "('lj1123.inktomisearch.com', 2)\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ssc.start() \n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://spark.apache.org/docs/latest/streaming-programming-guide.html#window-operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
