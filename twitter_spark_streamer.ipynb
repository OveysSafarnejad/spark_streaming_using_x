{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c864c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "\n",
    "os.environ['SPARK_HOME'] = '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d7293ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/14 11:48:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n",
      "/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "ssc = StreamingContext(sc, 10)\n",
    "sql_context = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decd7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 5555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43b136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = socket_stream.window(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9307ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ('tag', 'count')\n",
    "tweet = namedtuple('Tweet', fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c75574cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    lines.flatMap(lambda line: line.split(\" \"))\n",
    "    .filter(lambda word: word.lower().startswith(\"#\"))\n",
    "    .map(lambda hashtag: (hashtag.lower(), 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .map(lambda record: tweet(record[0], record[1]))\n",
    "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab81d0dc",
   "metadata": {},
   "source": [
    "# ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46be5e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04f311d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e15b8bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/14 11:49:31 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:49:31 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:49:31 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 1)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:49:31 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 1) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:49:31 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job\n",
      "23/11/14 11:49:31 ERROR JobScheduler: Error running job streaming job 1699958970000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 1) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:49:40 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:49:40 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:49:40 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 2)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:49:40 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 2) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:49:40 ERROR TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job\n",
      "23/11/14 11:49:40 ERROR JobScheduler: Error running job streaming job 1699958980000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 2) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:49:41 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/11/14 11:49:41 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/11/14 11:49:43 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:49:43 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:49:45 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:49:45 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:49:47 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:49:47 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:49:49 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:49:49 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:49:50 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:49:50 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:49:50 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 3)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:49:50 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 3) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:49:50 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job\n",
      "23/11/14 11:49:50 ERROR JobScheduler: Error running job streaming job 1699958990000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 3) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:49:51 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:49:51 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:49:53 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:49:53 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:49:55 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:49:55 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:49:57 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:49:57 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:49:59 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:49:59 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:00 ERROR Executor: Exception in task 0.0 in stage 8.0 (TID 4)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:00 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 4) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:50:00 ERROR TaskSetManager: Task 0 in stage 8.0 failed 1 times; aborting job\n",
      "23/11/14 11:50:00 ERROR JobScheduler: Error running job streaming job 1699959000000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 4) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:01 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:01 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:03 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:03 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:05 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:05 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:07 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:07 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:09 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:09 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:10 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:10 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:10 ERROR Executor: Exception in task 0.0 in stage 10.0 (TID 5)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:10 WARN TaskSetManager: Lost task 0.0 in stage 10.0 (TID 5) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:50:10 ERROR TaskSetManager: Task 0 in stage 10.0 failed 1 times; aborting job\n",
      "23/11/14 11:50:10 ERROR JobScheduler: Error running job streaming job 1699959010000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 5) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:11 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:11 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:13 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:13 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:15 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:15 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:17 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:17 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:19 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:19 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:20 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:20 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:20 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 6)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:20 WARN TaskSetManager: Lost task 0.0 in stage 12.0 (TID 6) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:50:20 ERROR TaskSetManager: Task 0 in stage 12.0 failed 1 times; aborting job\n",
      "23/11/14 11:50:20 ERROR JobScheduler: Error running job streaming job 1699959020000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 6) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:21 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:21 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:23 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:23 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:25 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:25 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:27 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:27 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:29 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:29 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:30 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:30 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:30 ERROR Executor: Exception in task 0.0 in stage 14.0 (TID 7)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:30 WARN TaskSetManager: Lost task 0.0 in stage 14.0 (TID 7) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:50:30 ERROR TaskSetManager: Task 0 in stage 14.0 failed 1 times; aborting job\n",
      "23/11/14 11:50:30 ERROR JobScheduler: Error running job streaming job 1699959030000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 7) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:31 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:31 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:33 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:33 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:35 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:35 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:37 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:37 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:39 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:39 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:40 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:40 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:40 ERROR Executor: Exception in task 0.0 in stage 16.0 (TID 8)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:40 WARN TaskSetManager: Lost task 0.0 in stage 16.0 (TID 8) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:50:40 ERROR TaskSetManager: Task 0 in stage 16.0 failed 1 times; aborting job\n",
      "23/11/14 11:50:40 ERROR JobScheduler: Error running job streaming job 1699959040000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 8) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:41 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:41 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:43 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:43 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:45 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:45 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:47 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:47 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:49 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:49 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:50 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:50 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:50 ERROR Executor: Exception in task 0.0 in stage 18.0 (TID 9)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:50:50 WARN TaskSetManager: Lost task 0.0 in stage 18.0 (TID 9) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:50:50 ERROR TaskSetManager: Task 0 in stage 18.0 failed 1 times; aborting job\n",
      "23/11/14 11:50:50 ERROR JobScheduler: Error running job streaming job 1699959050000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 9) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:51 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:51 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:53 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:53 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:55 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:55 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:57 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:57 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:50:59 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:50:59 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:00 ERROR Executor: Exception in task 0.0 in stage 20.0 (TID 10)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:00 WARN TaskSetManager: Lost task 0.0 in stage 20.0 (TID 10) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:51:00 ERROR TaskSetManager: Task 0 in stage 20.0 failed 1 times; aborting job\n",
      "23/11/14 11:51:00 ERROR JobScheduler: Error running job streaming job 1699959060000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 10) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:01 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:01 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:03 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:03 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:05 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:05 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:07 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:07 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:09 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:09 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:10 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:10 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:10 ERROR Executor: Exception in task 0.0 in stage 22.0 (TID 11)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:10 WARN TaskSetManager: Lost task 0.0 in stage 22.0 (TID 11) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:51:10 ERROR TaskSetManager: Task 0 in stage 22.0 failed 1 times; aborting job\n",
      "23/11/14 11:51:10 ERROR JobScheduler: Error running job streaming job 1699959070000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 11) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:11 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:11 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:13 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:13 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:15 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:15 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:17 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:17 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:19 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:19 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:20 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:20 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:20 ERROR Executor: Exception in task 0.0 in stage 24.0 (TID 12)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:20 WARN TaskSetManager: Lost task 0.0 in stage 24.0 (TID 12) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:51:20 ERROR TaskSetManager: Task 0 in stage 24.0 failed 1 times; aborting job\n",
      "23/11/14 11:51:20 ERROR JobScheduler: Error running job streaming job 1699959080000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 12) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:21 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:21 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:23 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:23 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:25 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:25 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:27 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:27 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:29 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:29 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:30 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:30 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:30 ERROR Executor: Exception in task 0.0 in stage 26.0 (TID 13)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:30 WARN TaskSetManager: Lost task 0.0 in stage 26.0 (TID 13) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:51:30 ERROR TaskSetManager: Task 0 in stage 26.0 failed 1 times; aborting job\n",
      "23/11/14 11:51:30 ERROR JobScheduler: Error running job streaming job 1699959090000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 13) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:31 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:31 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:33 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:33 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:35 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:35 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:37 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:37 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:39 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:39 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:40 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:40 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:40 ERROR Executor: Exception in task 0.0 in stage 28.0 (TID 14)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:40 WARN TaskSetManager: Lost task 0.0 in stage 28.0 (TID 14) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:51:40 ERROR TaskSetManager: Task 0 in stage 28.0 failed 1 times; aborting job\n",
      "23/11/14 11:51:40 ERROR JobScheduler: Error running job streaming job 1699959100000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 14) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:41 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:41 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:43 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:43 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:45 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:45 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:47 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:47 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:49 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:49 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:50 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:50 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:50 ERROR Executor: Exception in task 0.0 in stage 30.0 (TID 15)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:51:50 WARN TaskSetManager: Lost task 0.0 in stage 30.0 (TID 15) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:51:50 ERROR TaskSetManager: Task 0 in stage 30.0 failed 1 times; aborting job\n",
      "23/11/14 11:51:50 ERROR JobScheduler: Error running job streaming job 1699959110000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 15) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:51 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:51 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:53 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:53 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:55 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:55 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:57 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:57 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:51:59 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:51:59 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:00 ERROR Executor: Exception in task 0.0 in stage 32.0 (TID 16)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:00 WARN TaskSetManager: Lost task 0.0 in stage 32.0 (TID 16) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:52:00 ERROR TaskSetManager: Task 0 in stage 32.0 failed 1 times; aborting job\n",
      "23/11/14 11:52:00 ERROR JobScheduler: Error running job streaming job 1699959120000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 16) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:01 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:01 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:03 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:03 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:05 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:05 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:07 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:07 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:09 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:09 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:10 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:10 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:10 ERROR Executor: Exception in task 0.0 in stage 34.0 (TID 17)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:10 WARN TaskSetManager: Lost task 0.0 in stage 34.0 (TID 17) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:52:10 ERROR TaskSetManager: Task 0 in stage 34.0 failed 1 times; aborting job\n",
      "23/11/14 11:52:10 ERROR JobScheduler: Error running job streaming job 1699959130000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 17) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:11 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:11 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:13 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:13 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:15 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:15 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:17 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:17 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:19 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:19 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:20 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:20 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:20 ERROR Executor: Exception in task 0.0 in stage 36.0 (TID 18)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:20 WARN TaskSetManager: Lost task 0.0 in stage 36.0 (TID 18) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:52:20 ERROR TaskSetManager: Task 0 in stage 36.0 failed 1 times; aborting job\n",
      "23/11/14 11:52:20 ERROR JobScheduler: Error running job streaming job 1699959140000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 18) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:21 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:21 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:23 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:23 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:25 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:25 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:27 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:27 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:29 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:29 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:30 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:30 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:30 ERROR Executor: Exception in task 0.0 in stage 38.0 (TID 19)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:30 WARN TaskSetManager: Lost task 0.0 in stage 38.0 (TID 19) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:52:30 ERROR TaskSetManager: Task 0 in stage 38.0 failed 1 times; aborting job\n",
      "23/11/14 11:52:30 ERROR JobScheduler: Error running job streaming job 1699959150000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 19) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:31 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:31 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:33 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:33 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:35 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:35 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:37 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:37 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:39 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:39 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:40 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:40 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:40 ERROR Executor: Exception in task 0.0 in stage 40.0 (TID 20)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:40 WARN TaskSetManager: Lost task 0.0 in stage 40.0 (TID 20) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:52:40 ERROR TaskSetManager: Task 0 in stage 40.0 failed 1 times; aborting job\n",
      "23/11/14 11:52:40 ERROR JobScheduler: Error running job streaming job 1699959160000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 1 times, most recent failure: Lost task 0.0 in stage 40.0 (TID 20) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:41 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:41 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:43 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:43 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:45 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:45 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:47 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:47 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:49 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:49 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:50 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:50 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:50 ERROR Executor: Exception in task 0.0 in stage 42.0 (TID 21)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:52:50 WARN TaskSetManager: Lost task 0.0 in stage 42.0 (TID 21) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:52:50 ERROR TaskSetManager: Task 0 in stage 42.0 failed 1 times; aborting job\n",
      "23/11/14 11:52:50 ERROR JobScheduler: Error running job streaming job 1699959170000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 1 times, most recent failure: Lost task 0.0 in stage 42.0 (TID 21) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:51 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:51 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:53 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:53 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:55 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:55 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:57 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:57 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:52:59 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:52:59 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:53:00 ERROR PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 810, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:53:00 ERROR PythonRunner: This may have been caused by a prior exception:\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:53:00 ERROR Executor: Exception in task 0.0 in stage 44.0 (TID 22)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/11/14 11:53:00 WARN TaskSetManager: Lost task 0.0 in stage 44.0 (TID 22) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "23/11/14 11:53:00 ERROR TaskSetManager: Task 0 in stage 44.0 failed 1 times; aborting job\n",
      "23/11/14 11:53:00 ERROR JobScheduler: Error running job streaming job 1699959180000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/var/folders/g6/vgc6wxj13x95m3zxhrn480540000gn/T/ipykernel_13064/819874052.py\", line 7, in <lambda>\n",
      "    .foreachRDD(lambda rdd: rdd.toDF().sort(desc('count')).limit(10).registerTempTable('tweets'))\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 122, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1443, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1483, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1056, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 996, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2888, in first\n",
      "    rs = self.take(1)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/rdd.py\", line 2855, in take\n",
      "    res = self.context.runJob(self, takeUpToNumLeft, p)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/context.py\", line 2510, in runJob\n",
      "    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/.venv/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 1 times, most recent failure: Lost task 0.0 in stage 44.0 (TID 22) (safarnejad executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\n",
      "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy33.call(Unknown Source)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 812, in main\n",
      "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "AttributeError: Can't get attribute 'PySparkRuntimeError' on <module 'pyspark.errors.exceptions.base' from '/Users/audioworkstation/Documents/WORKSPACE/LEARNING/spark_streaming_using_x/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/base.py'>\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:53:01 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:53:01 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:53:03 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:53:03 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:53:05 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:53:05 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n",
      "23/11/14 11:53:07 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:5555\n",
      "java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "23/11/14 11:53:07 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:5555 - java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.connect0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:579)\n",
      "\tat java.base/sun.nio.ch.Net.connect(Net.java:568)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:593)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:583)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:507)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:287)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a57036",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "while count < 10:\n",
    "    time.sleep(3)\n",
    "    top_10 = sql_context.sql(\"SELECT tag, count from tweets\")\n",
    "    top_10_pd_df = top_10.toPandas()\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize= (10, 8))\n",
    "    sns.barplot(x='count', y='tag', data=top_10_pd_df)\n",
    "    plt.show()\n",
    "    count = count + 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215db88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
